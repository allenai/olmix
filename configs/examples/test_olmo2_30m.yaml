# Minimal test configuration for olmo2_30m model
# Use this to verify the launch pipeline works
#
# Usage:
#   olmix launch preview --config configs/examples/test_olmo2_30m.yaml
#   olmix launch run --config configs/examples/test_olmo2_30m.yaml --dry-run

name: olmix-test-30m
description: Test 30M model training (minimal run)
budget: ai2/oe-data
workspace: ai2/dolma2
cluster: ai2/saturn-cirrascale

# Model settings
proxy_model_id: olmo2_30m
tokenizer: dolma2

# Training settings (Chinchilla-based)
# chinchilla_multiple: N means train for N * 20 * params tokens
# For 30M model: 0.017 * 20 * 30M = 10M tokens (minimal test)
chinchilla_multiple: 0.017
nodes: 1
gpus: 1
variants: 2
seed: 42

# Data sources for testing (dolma2-tokenized)
sources:
  - name: dclm
    max_repetition_factor: 1.0
    topics:
      - name: science_math_and_technology
        paths:
          - "s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/science_math_and_technology/**/*.npy"
      - name: software_development
        paths:
          - "s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/software_development/**/*.npy"
      - name: education_and_jobs
        paths:
          - "s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/education_and_jobs/**/*.npy"

  - name: wikipedia
    paths:
      - "s3://ai2-llm/preprocessed/wikipedia-dolma-0823/allenai/dolma2-tokenizer/*.npy"
    max_repetition_factor: 2.0

  - name: arxiv
    paths:
      - "s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated-0625_tokenized/arxiv/train/allenai/dolma2-tokenizer/*.npy"
    max_repetition_factor: 1.5
