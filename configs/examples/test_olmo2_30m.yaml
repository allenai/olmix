# Minimal test configuration for olmo2_30m model
# Use this to verify the launch pipeline works
#
# Usage:
#   olmix launch preview --config configs/examples/test_olmo2_30m.yaml
#   olmix launch run --config configs/examples/test_olmo2_30m.yaml --dry-run

name: olmix-test-30m
description: Test 30M model training (minimal run)
budget: ai2/oe-base
workspace: ai2/oe-data
cluster: ai2/jupiter
priority: high

# Model settings
proxy_model_id: olmo2_30m
tokenizer: dolma2

# Training settings (Chinchilla-based)
# chinchilla_multiple: N means train for N * 20 * params tokens
# For 30M model: 0.5 * 20 * 30M = 300M tokens
chinchilla_multiple: 0.5
nodes: 1
gpus: 1
# Override batch size for small model test (default Chinchilla formula gives 262144 which is too large)
# 16 sequences * 8192 tokens = 131k tokens/batch -> ~2200 steps
global_batch_size: 16
variants: 2
seed: 42

# Final run constraint settings (for olmix fit --constrain-objective)
# target_tokens = target_chinchilla_multiple * 20 * target_model_params
# For 7B model with 20x Chinchilla: 20 * 20 * 7B = 2.8T tokens
target_model_id: olmo2_7b
target_chinchilla_multiple: 20.0
repetition_factor: 5.0

# Data sources for testing (dolma2-tokenized)
sources:
  - name: dclm
    max_repetition_factor: 1.0
    topics:
      - name: science_math_and_technology
        paths:
          - "s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/science_math_and_technology/**/*.npy"
      - name: software_development
        paths:
          - "s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/software_development/**/*.npy"
      - name: education_and_jobs
        paths:
          - "s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/education_and_jobs/**/*.npy"

  - name: wikipedia
    paths:
      - "s3://ai2-llm/preprocessed/wikipedia-dolma-0823/allenai/dolma2-tokenizer/*.npy"
    max_repetition_factor: 2.0

  - name: arxiv
    paths:
      - "s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated-0625_tokenized/arxiv/train/allenai/dolma2-tokenizer/*.npy"
    max_repetition_factor: 1.5
