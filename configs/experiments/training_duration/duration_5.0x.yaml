# Training Duration Experiment: 5.0x Chinchilla
# Uses 14M model with 5.0 chinchilla_multiple
# Expected tokens: 5.0 * 20 * 14M = 1.4B tokens
# Estimated runtime: ~70 min (1 GPU on Jupiter)
#
# Usage:
#   olmix launch run --config configs/experiments/training_duration/duration_5.0x.yaml

name: duration-5.0x
description: Training duration experiment - 5.0x Chinchilla (1.4B tokens)
infra:
  budget: ai2/oe-base
  workspace: ai2/oe-data
  cluster: ai2/jupiter
  priority: high
  nodes: 1
  gpus: 1
training:
  proxy_model_id: olmo3_14m
  tokenizer: dolma2
  chinchilla_multiple: 5.0
  seed: 42
  global_batch_size: 16
data:
  sources:
  - name: dclm
    max_repetition_factor: 1.0
    topics:
    - name: science_math_and_technology
      paths:
      - s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/science_math_and_technology/**/*.npy
    - name: software_development
      paths:
      - s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/software_development/**/*.npy
    - name: education_and_jobs
      paths:
      - s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/education_and_jobs/**/*.npy
  - name: wikipedia
    paths:
    - s3://ai2-llm/preprocessed/wikipedia-dolma-0823/allenai/dolma2-tokenizer/*.npy
    max_repetition_factor: 2.0
  - name: arxiv
    paths:
    - s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated-0625_tokenized/arxiv/train/allenai/dolma2-tokenizer/*.npy
    max_repetition_factor: 1.5
swarm:
  seed: 42
  variants: 1
