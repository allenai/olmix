# Data Proportions Experiment: Science-Heavy Mix
# Uses 14M model with 0.5 chinchilla_multiple
# Expected tokens: 0.5 * 20 * 14M = 140M tokens
# Estimated runtime: ~8 min (1 GPU on Jupiter)
#
# Target proportions:
#   - science_math_and_technology: 50%
#   - software_development: 10%
#   - education_and_jobs: 10%
#   - wikipedia: 10%
#   - arxiv: 20%
#
# Usage:
#   olmix launch run --config configs/experiments/data_proportions/mix_heavy_science.yaml

name: mix-heavy-science
description: Data proportions experiment - 50% science emphasis
budget: ai2/oe-base
workspace: ai2/oe-data
cluster: ai2/jupiter
priority: high

# Model settings - use 14M model for fast tests
proxy_model_id: olmo3_14m
tokenizer: dolma2

# Training settings - fixed at 0.5x Chinchilla for all proportion experiments
chinchilla_multiple: 0.5  # 0.5 * 20 * 14M = 140M tokens
nodes: 1
gpus: 1
global_batch_size: 16
variants: 1
seed: 42

# Data sources - science-heavy mix
# Target: 50% science, 10% code, 10% education, 10% wiki, 20% arxiv
sources:
  - name: dclm
    max_repetition_factor: 1.0
    topics:
      - name: science_math_and_technology
        weight: 5.0  # 50% - primary emphasis
        paths:
          - "s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/science_math_and_technology/**/*.npy"
      - name: software_development
        weight: 1.0  # 10%
        paths:
          - "s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/software_development/**/*.npy"
      - name: education_and_jobs
        weight: 1.0  # 10%
        paths:
          - "s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/education_and_jobs/**/*.npy"

  - name: wikipedia
    weight: 1.0  # 10%
    paths:
      - "s3://ai2-llm/preprocessed/wikipedia-dolma-0823/allenai/dolma2-tokenizer/*.npy"
    max_repetition_factor: 2.0

  - name: arxiv
    weight: 2.0  # 20% - complementary to science
    paths:
      - "s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated-0625_tokenized/arxiv/train/allenai/dolma2-tokenizer/*.npy"
    max_repetition_factor: 1.5
