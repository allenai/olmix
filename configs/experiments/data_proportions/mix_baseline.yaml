# Data Proportions Experiment: Baseline Mix
# Uses 14M model with 0.5 chinchilla_multiple
# Expected tokens: 0.5 * 20 * 14M = 140M tokens
# Estimated runtime: ~8 min (1 GPU on Jupiter)
#
# This is the control config with balanced proportions across all domains.
#
# Usage:
#   olmix launch run --config configs/experiments/data_proportions/mix_baseline.yaml

name: mix-baseline
description: Data proportions experiment - balanced baseline mix
infra:
  budget: ai2/oe-base
  workspace: ai2/oe-data
  cluster: ai2/jupiter
  priority: high
  nodes: 1
  gpus: 1
training:
  proxy_model_id: olmo3_14m
  tokenizer: dolma2
  chinchilla_multiple: 0.5
  seed: 42
  global_batch_size: 16
data:
  sources:
  - name: dclm
    max_repetition_factor: 1.0
    topics:
    - name: science_math_and_technology
      paths:
      - s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/science_math_and_technology/**/*.npy
    - name: software_development
      paths:
      - s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/software_development/**/*.npy
    - name: education_and_jobs
      paths:
      - s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/education_and_jobs/**/*.npy
  - name: wikipedia
    paths:
    - s3://ai2-llm/preprocessed/wikipedia-dolma-0823/allenai/dolma2-tokenizer/*.npy
    max_repetition_factor: 2.0
  - name: arxiv
    paths:
    - s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated-0625_tokenized/arxiv/train/allenai/dolma2-tokenizer/*.npy
    max_repetition_factor: 1.5
swarm:
  seed: 42
  variants: 1
