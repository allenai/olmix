# Data Proportions Experiment: Code-Heavy Mix
# Uses 14M model with 0.5 chinchilla_multiple
# Expected tokens: 0.5 * 20 * 14M = 140M tokens
# Estimated runtime: ~8 min (1 GPU on Jupiter)
#
# Target proportions:
#   - software_development: 50%
#   - science_math_and_technology: 20%
#   - education_and_jobs: 10%
#   - wikipedia: 10%
#   - arxiv: 10%
#
# Usage:
#   olmix launch run --config configs/experiments/data_proportions/mix_heavy_code.yaml

name: mix-heavy-code
description: Data proportions experiment - 50% code emphasis
infra:
  budget: ai2/oe-base
  workspace: ai2/oe-data
  cluster: ai2/saturn
  priority: urgent
  nodes: 1
  gpus: 1
training:
  proxy_model_id: olmo3_14m
  tokenizer: dolma2
  chinchilla_multiple: 0.5
  seed: 42
  global_batch_size: 16
data:
  sources:
  - name: dclm
    max_repetition_factor: 1.0
    topics:
    - name: software_development
      weight: 5.0
      paths:
      - s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/software_development/**/*.npy
    - name: science_math_and_technology
      weight: 2.0
      paths:
      - s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/science_math_and_technology/**/*.npy
    - name: education_and_jobs
      weight: 1.0
      paths:
      - s3://ai2-llm/preprocessed/dclm/baseline_topic_ft_lr05_ng2_n3M6_ova_20pct/allenai/dolma2-tokenizer/education_and_jobs/**/*.npy
  - name: wikipedia
    weight: 1.0
    paths:
    - s3://ai2-llm/preprocessed/wikipedia-dolma-0823/allenai/dolma2-tokenizer/*.npy
    max_repetition_factor: 2.0
  - name: arxiv
    weight: 1.0
    paths:
    - s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated-0625_tokenized/arxiv/train/allenai/dolma2-tokenizer/*.npy
    max_repetition_factor: 1.5
swarm:
  seed: 42
  variants: 1
